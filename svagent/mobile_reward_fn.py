# pylint: disable=line-too-long, function-name-too-long
import os
import re
import json
import asyncio
from rich import print
from datetime import datetime
from openai import AsyncOpenAI
from svagent.templates.verifier_prompt import SYSTEM_PROMPT_VERIFIER
from svagent.mobile_tool_agent_loop import ToolAgentLoop
from typing import Tuple, List
import numpy as np



def parse_solution_str(solution_str, messages=[]):
    """
    ä» solution_str ä¸­è§£æå„ç§ä¿¡æ¯
    
    Returns:
        dict: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸
        - evidences: list of evidence XML strings
        - valid_evidence: bool or None (ä»<ValidEvidence>æ ‡è®°è§£æ)
        - verdict: str or None (ä»<Verdict>æ ‡è®°è§£æï¼ŒSUCCESS/FAILURE)
        - last_tool_is_submit: bool (æœ€åä¸€è½®å·¥å…·è°ƒç”¨æ˜¯å¦ä¸ºsubmit)
        - submit_call_count: int (submitå·¥å…·è¢«è°ƒç”¨çš„æ¬¡æ•°)
        - submit_message: str or None (submitå·¥å…·çš„messageå‚æ•°)
        - submit_evidences: list or None (submitå·¥å…·çš„evidenceså‚æ•°)
        - submit_evidences_valid: bool (submitä¸­çš„evidencesä¸‹æ ‡æ˜¯å¦éƒ½å­˜åœ¨)
        - round_analysis_valid: bool (æ˜¯å¦æ¯ä¸ªtool_callå‰éƒ½æœ‰Round Xå’ŒAnalysis)
        - round_sequence_valid: bool (Round Xçš„åºå·æ˜¯å¦æŒ‰é¡ºåºé€’å¢)
    """
    result = {
        'evidences': [],
        'valid_evidence': None,
        'verdict': None,
        'last_tool_is_submit': False,
        'submit_call_count': 0,
        'submit_message': None,
        'submit_evidences': None,
        'submit_evidences_valid': False,
        'round_analysis_valid': False,
        'round_sequence_valid': False
    }
    
    # 1. è§£æ evidences (tool_responseä¸­çš„XML)
    # è¿™é‡Œéœ€è¦è€ƒè™‘åˆ°ä¸åŒæ¨¡å‹è¾“å‡º ä¸ä¸€å®šéƒ½æ˜¯<tool_response>...</tool_response>æ¥åŒ…è£¹çš„
    # tool_response_pattern = r'<tool_response>(.*?)</tool_response>'
    # ç”¨<observation>...</observation>æ¥æ‰¾æ˜¯æœ€å¥½çš„
    if not ("<observation>" in solution_str and "</observation>" in solution_str):
        print("ğŸš¨>>> é‡åˆ°äº†æ²¡æœ‰ä»»ä½•å·¥å…·è°ƒç”¨çš„è¾“å‡ºç»“æœ", solution_str)
    tool_response_pattern = r'<observation>(.*?)</observation>'
    tool_responses = re.findall(tool_response_pattern, solution_str, re.DOTALL)
    result['evidences'] = [response.strip() for response in tool_responses]
    
    # 2. è§£æ ValidEvidence æ ‡è®°
    valid_evidence_match = re.search(r'<ValidEvidence>\s*\[?(True|False)\]?\s*</ValidEvidence>', solution_str, re.IGNORECASE)
    if valid_evidence_match:
        result['valid_evidence'] = valid_evidence_match.group(1).lower() == 'true'
    
    # 3. è§£æ Verdict æ ‡è®°
    verdict_match = re.search(r'<Verdict>\s*\[?(SUCCESS|FAILURE)\]?\s*</Verdict>', solution_str, re.IGNORECASE)
    if verdict_match:
        result['verdict'] = verdict_match.group(1).upper()
    
    # 4. è§£æå·¥å…·è°ƒç”¨ï¼Œæ£€æŸ¥æœ€åä¸€ä¸ªæ˜¯å¦ä¸ºsubmit
    parsed_tool_calls_messages = []
    if len(messages):
        for message in messages:
            if "tool_calls" in message:
                tool_calls = message["tool_calls"]
                for tool_call in tool_calls:
                    if type(tool_call) is dict:
                        name = tool_call["function"]["name"]
                        arguments = tool_call["function"]["arguments"]
                    else:
                        name = tool_call.name
                        arguments = tool_call.arguments
                    parsed_tool_calls_messages.append({
                        "name": name,
                        "arguments": arguments,
                    })

    tool_call_pattern = r'<tool_call>\s*(.*?)\s*</tool_call>'
    tool_call_matches = re.findall(tool_call_pattern, solution_str, re.DOTALL)
    
    parsed_tool_calls = []
    for tool_call_str in tool_call_matches:
        try:
            # ä½¿ç”¨evalå°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå­—å…¸
            tool_call_dict = eval(tool_call_str.strip())
            if isinstance(tool_call_dict, dict) and 'name' in tool_call_dict:
                parsed_tool_calls.append(tool_call_dict)
        except (SyntaxError, NameError, ValueError, TypeError):
            # å¦‚æœevalå¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªå·¥å…·è°ƒç”¨    
            continue
    print("ğŸ“© >>> messages: ", messages, ">>> ğŸ› ï¸ parsed_tool_calls: ", parsed_tool_calls, ">>> ğŸ› ï¸ parsed_tool_calls_messages: ", parsed_tool_calls_messages)
    if len(parsed_tool_calls) == 0:
        parsed_tool_calls = parsed_tool_calls_messages
    
    if parsed_tool_calls:
        # ç»Ÿè®¡ submit å·¥å…·è°ƒç”¨æ¬¡æ•°
        result['submit_call_count'] = sum(1 for call in parsed_tool_calls if call.get('name') == 'submit')

        # è·å–æœ€åä¸€ä¸ªå·¥å…·è°ƒç”¨çš„åç§°
        last_tool_call = parsed_tool_calls[-1]
        result['last_tool_is_submit'] = last_tool_call.get('name') == 'submit'
        
        # å¦‚æœæœ€åä¸€ä¸ªæ˜¯submitï¼Œè§£æå…¶å‚æ•°
        if result['last_tool_is_submit']:
            try:
                args = last_tool_call.get('arguments', {})
                if type(args) is str:
                    try:
                        args = json.loads(args)
                    except Exception as e:
                        print(f"loading args {args} for reward calculation failed {e}")
                        args = {}
                result['submit_message'] = args.get('message')
                result['submit_evidences'] = args.get('evidences', [])
                
                # æ£€æŸ¥evidencesä¸‹æ ‡æ˜¯å¦éƒ½å­˜åœ¨ï¼Œå¹¶ä¸”å¯¹åº”çš„å·¥å…·è°ƒç”¨ä¸æ˜¯ 'submit'
                if result['submit_evidences']:
                    max_evidence_idx = len(result['evidences']) - 1
                    result['submit_evidences_valid'] = all(
                        isinstance(idx, int) and 0 <= idx <= max_evidence_idx and
                        parsed_tool_calls[idx].get('name') != 'submit'
                        for idx in result['submit_evidences']
                    )
                else:
                    result['submit_evidences_valid'] = False
                    
            except (KeyError, TypeError):
                pass
    
    # 5. è§£æRound Xå’ŒAnalysisæ ¼å¼
    result["round_analysis_valid"] = True
    result["round_sequence_valid"] = True
    return result


async def single_verification(bot, messages, model_name, stream, thinking):
    """å•æ¬¡éªŒè¯å‡½æ•°ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    max_retries = 3
    retry_delay = 20  # seconds

    last_exception = None
    for attempt in range(max_retries):
        try:
            response_stream = await bot.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=0.7, 
                stream=stream,
                extra_body={
                    "enable_thinking": thinking, 
                    "chat_template_kwargs": {"thinking": thinking}, 
                    "separate_reasoning": True, 
                }
            )
            response_plain_text = ""
            async for chunk in response_stream:
                if len(chunk.choices) == 0:
                    continue
                if stream and thinking and hasattr(chunk.choices[0].delta, "reasoning_content") and chunk.choices[0].delta.reasoning_content:
                    response_plain_text += chunk.choices[0].delta.reasoning_content
                if stream and chunk.choices[0].delta.content:
                    response_plain_text += chunk.choices[0].delta.content
            
            verdict_match = re.search(r'<Verdict>\s*\[?(SUCCESS|FAILURE)\]?\s*</Verdict>', response_plain_text, re.IGNORECASE)
            valid_evidence_match = re.search(r'<ValidEvidence>\s*\[?(True|False)\]?\s*</ValidEvidence>', response_plain_text, re.IGNORECASE)
            
            success = False
            valid_evidence = False
            
            if verdict_match:
                verdict = verdict_match.group(1).upper()
                success = "SUCCESS" in verdict
            
            if valid_evidence_match:
                evidence_value = valid_evidence_match.group(1).lower()
                valid_evidence = evidence_value == "true"
            
            # å¦‚æœæˆåŠŸè§£æï¼Œåˆ™ç›´æ¥è¿”å›ç»“æœ
            return success, valid_evidence, response_plain_text

        except Exception as e:
            last_exception = e
            print(f"éªŒè¯å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}")
            if attempt < max_retries - 1:
                print(f"å°†åœ¨ {retry_delay} ç§’åé‡è¯•...")
                await asyncio.sleep(retry_delay)
            else:
                print("å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ŒéªŒè¯å¤±è´¥ã€‚")
    
    # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œåˆ™æŠ›å‡ºæœ€åä¸€æ¬¡çš„å¼‚å¸¸
    raise last_exception



def save_verification_results(save_dir, task_instruction, finish_message, submit_evidences, 
                            messages, model_name, results, N):
    """ä¿å­˜éªŒè¯ç»“æœåˆ°æŒ‡å®šç›®å½•"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    verification_results_dir = os.path.join(save_dir, f"verification_results_{timestamp}")
    os.makedirs(verification_results_dir, exist_ok=True)
    
    # ä¿å­˜å®Œæ•´çš„ messages å’Œæ¯æ¬¡éªŒè¯çš„ç»“æœ
    verification_data = {
        "task_instruction": task_instruction,
        "finish_message": finish_message,
        "submit_evidences": submit_evidences,
        "messages": messages,
        "model_name": model_name,
        "timestamp": timestamp,
        "verification_results": []
    }
    
    # ç»Ÿè®¡ç»“æœå¹¶æ„å»ºéªŒè¯æ•°æ®
    success_count = 0
    valid_evidence_count = 0
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            result_data = {
                "verification_index": i + 1,
                "success": False,
                "exception": str(result),
                "error": True,
                "response_text": None
            }
            print(f"ç¬¬{i+1}æ¬¡éªŒè¯å‡ºç°å¼‚å¸¸: {result}")
        else:
            success, valid_evidence, response_text = result
            result_data = {
                "verification_index": i + 1,
                "success": success,
                "valid_evidence": valid_evidence,
                "response_text": response_text,
                "error": False
            }
            if success:
                success_count += 1
            if valid_evidence:
                valid_evidence_count += 1
            print(f"ç¬¬{i+1}æ¬¡éªŒè¯ç»“æœ: {'SUCCESS' if success else 'FAILURE'}, ValidEvidence: {'True' if valid_evidence else 'False'}")
        
        verification_data["verification_results"].append(result_data)
    
    # ä¿å­˜éªŒè¯æ•°æ®åˆ°æ–‡ä»¶
    verification_file = os.path.join(verification_results_dir, "verification_report.json")
    with open(verification_file, 'w', encoding='utf-8') as f:
        json.dump(verification_data, f, ensure_ascii=False, indent=2)
    
    # å¦å¤–ä¿å­˜æ¯ä¸ªéªŒè¯çš„è¯¦ç»†å“åº”åˆ°å•ç‹¬æ–‡ä»¶
    for i, result_data in enumerate(verification_data["verification_results"]):
        if not result_data["error"] and result_data["response_text"]:
            response_file = os.path.join(verification_results_dir, f"verification_{i+1}_response.txt")
            with open(response_file, 'w', encoding='utf-8') as f:
                f.write(result_data["response_text"])
    
    print(f"éªŒè¯ç»“æœå·²ä¿å­˜åˆ°: {verification_results_dir}")
    return success_count, valid_evidence_count, verification_results_dir




def compute_score(data_source, solution_str, ground_truth, extra_info=None, **kwargs):
    # ground_truth è¡¨ç¤ºçš„æ˜¯ task instruction
    # solution æ˜¯å®Œæ•´çš„å›ç­”
    # extra_info ä¸­åŒ…å«å†å²è§‚æµ‹ã€æ—¥å¿—ç›®å½•ç­‰ä¿¡æ¯
    task_config = extra_info['task_config']
    task_instruction = task_config['task_instruction']
    step = extra_info.get("step", None)
    sample_index = extra_info.get("sample_index", None)
    rollout_n = extra_info.get("rollout_n", None)
    validate = extra_info.get("validate", None)
    # save_dir = os.environ.get('MOBILE_SESSION_SAVE_DIR') + "/response_logs"
    save_dir = extra_info["default_local_dir"]
    save_dir = os.path.join(save_dir, "response_logs")
    messages = kwargs["messages"]
    # tools = kwargs["tools"]
    use_toolcall_reward = kwargs["use_toolcall_reward"]
    max_toolcall_steps = kwargs["max_toolcall_steps"]
    global_steps = kwargs["global_steps"]
    if validate is not None and validate:
        save_dir = os.path.join(save_dir, "validation")
    else:
        save_dir = os.path.join(save_dir, "training")
    if step is not None:
        save_dir = os.path.join(save_dir, f"step_{step}")
    # if sample_index and rollout_n:
    #     save_dir = os.path.join(save_dir, f"sample_index_{sample_index}_rollout_n_{rollout_n}")
    os.makedirs(save_dir, exist_ok=True)

    # ä¿å­˜ solution_str åˆ° txt æ–‡ä»¶
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    solution_file = os.path.join(save_dir, f"solution_{timestamp}.txt")
    with open(solution_file, 'w', encoding='utf-8') as f:
        f.write(solution_str)

    # è§£æ solution_str ä¸­çš„å„ç§ä¿¡æ¯
    parsed_info = parse_solution_str(solution_str, messages)
    # æœ‰å¤šå°‘æ¬¡tool responseå°±å¯¹åº”ç€å¤šå°‘æ¬¡tool call
    num_toolcalls = len(parsed_info['evidences'])
    # åˆå§‹å¥–åŠ±åˆ†æ•°
    total_reward = 0.0
    # æ£€æŸ¥åŸºæœ¬æ ¼å¼å’Œå®Œæˆåº¦
    format_penalties = 0.0

    # 1. æ£€æŸ¥Roundæ ¼å¼
    # if not parsed_info['round_analysis_valid']:
    #     print("æƒ©ç½šï¼šå­˜åœ¨ tool_call å‰ç¼ºå°‘ Round X å’Œ Analysis æ ¼å¼")
    #     format_penalties += 1.0
    
    # if not parsed_info['round_sequence_valid']:
    #     print("æƒ©ç½šï¼šRoundåºå·ä¸æ˜¯æŒ‰é¡ºåºé€’å¢")
    #     format_penalties += 0.5

    # 2. æ£€æŸ¥æ˜¯å¦è°ƒç”¨äº†submitå·¥å…·, æ£€æŸ¥submitçš„evidenceså‚æ•°æ˜¯å¦æœ‰æ•ˆ
    if parsed_info['submit_call_count'] != 1: 
        format_penalties += 1.0
        print(f"[blue]<X> æƒ©ç½šï¼šsubmit è°ƒç”¨æ¬¡æ•° {parsed_info['submit_call_count']} != 1, å¥–åŠ±: {-format_penalties}[/blue]")
        res = {
            "score": - format_penalties,
            "acc": 0.0,
        }
        return res
    if not parsed_info['last_tool_is_submit']:
        format_penalties += 1.0
        print(f"[blue]<X> æƒ©ç½šï¼šæœ€åä¸€è½®å·¥å…·è°ƒç”¨ä¸æ˜¯ submit, å¥–åŠ±: {-format_penalties}[/blue]")
        res = {
            "score": - format_penalties,
            "acc": 0.0,
        }
        return res
    if not parsed_info['submit_evidences_valid']:
        format_penalties += 1.0
        print(f"[blue]<X> æƒ©ç½šï¼šè°ƒç”¨ submit ä¼ å…¥çš„è¯æ®å‚æ•°ä¸å¯¹, å¥–åŠ±: {-format_penalties}[/blue]")
        res = {
            "score": - format_penalties,
            "acc": 0.0,
        }
        return res

    # æ„å»ºéªŒè¯æ¶ˆæ¯
    messages = [{'role': 'user', 'content': SYSTEM_PROMPT_VERIFIER + f"\n## Task Description\n{task_instruction}\n\n## Agent's Final Claim\n{parsed_info['submit_message'] or 'No message provided'}"}]
    
    # æ·»åŠ evidenceåˆ°æ¶ˆæ¯ä¸­
    for idx in parsed_info['submit_evidences']:
        if 0 <= idx < len(parsed_info['evidences']):
            evidence_content = parsed_info['evidences'][idx]
            messages.append({'role': 'user', 'content': f"** Round {idx} XML Evidence **\n{evidence_content}"})
    
    # model_name="qwen3-235b-a22b"
    model_name = os.environ.get("MODEL_NAME", "DeepSeek-V3.1")
    stream = True
    thinking = True
    if 'r1' in model_name: #! only deepseek-R1 ä¸æ”¯æŒè®¾ç½® thinking
        thinking = False
    
    # å¹¶è¡Œæµ‹è¯• N æ¬¡
    N = 3
    async def run_parallel_tests():
        # ä½¿ç”¨å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿å®¢æˆ·ç«¯æ­£ç¡®å…³é—­
        async with AsyncOpenAI(
            api_key=os.getenv("API_KEY"),
            base_url=os.getenv("BASE_URL"), 
            # base_url="https://dashscope.aliyuncs.com/compatible-mode/v1", 
            # api_key="sk-A7ViN2cT7MXHuzsj0jtXWt6WMe9xqEN77bVgQoW9OfYb0FiQ", 
            # base_url="https://api.kksj.org/v1",
        ) as bot:
            tasks = []
            for i in range(N):
                task = single_verification(bot, messages, model_name, stream, thinking)
                tasks.append(task)
            return await asyncio.gather(*tasks, return_exceptions=True)
    
    # è¿è¡Œå¹¶è¡Œæµ‹è¯•
    results = asyncio.run(run_parallel_tests())
    
    # ä¿å­˜éªŒè¯ç»“æœ
    success_count, valid_evidence_count, verification_results_dir = save_verification_results(
        save_dir, task_instruction, parsed_info['submit_message'] or 'No message provided', 
        parsed_info['submit_evidences'], messages, model_name, results, N
    )
    # æäº¤è¯æ®çš„æ¬¡æ•°ä½œä¸ºæƒ©ç½š; è¯æ®è¶…è¿‡3ä¸ªå°±ç»™ä¸€äº›æƒ©ç½š
    evidence_freq_penalties = 0.1 * max(len(parsed_info['submit_evidences']) - 3, 0)  # è¯æ®ä¸å®œå¤š å‡è½»LLM-as-a-Judgeå‹åŠ›

    # è®¡ç®—å¥–åŠ±åˆ†æ•°
    cnt_success = 0
    cnt_valid = 0
    no_exception = 0
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            continue
        else:
            success, valid_evidence, response_text = result
            if success:
                cnt_success += 1
            if valid_evidence:
                cnt_valid += 1
            no_exception += 1
    acc = 0.0
    if cnt_success * 2 > no_exception:
        verification_reward = 1.0
        acc = 1.0

    elif cnt_valid * 2 > no_exception:
        verification_reward = 0
    else:
        verification_reward = -1.0

    # è¿˜åº”è¯¥å¼•å…¥tool-call reward
    if verification_reward > 0:
        # å¦‚æœæ˜¯æ­£æ ·æœ¬ è¯æ®é“¾å®Œæ•´ä¸”ç¡®å‡¿->ä»…æœ‰æ— åŒºåˆ«
        toolcall_reward = float(num_toolcalls > 0)
    else:
        # å¦‚æœæ˜¯è´Ÿæ ·æœ¬ è¯æ®é“¾ä¸å®Œæ•´æˆ–ä¸æ­£ç¡®
        toolcall_reward = max(min(float(num_toolcalls)*0.1, 1.0), 0.0)
    assert global_steps >= 0
    if use_toolcall_reward == "none":
        res_toolcall_ratio = 0.0
    elif use_toolcall_reward == "constant":
        res_toolcall_ratio = 1
    elif use_toolcall_reward == "cosine":
        res_toolcall_ratio = 1
        if global_steps <= max_toolcall_steps:
            res_toolcall_ratio *= (np.cos((global_steps / max_toolcall_steps) * np.pi) + 1) / 2
        else:
            res_toolcall_ratio = 0
    else:
        raise NotImplementedError("use_toolcall_reward must be one of ['none', 'constant', 'cosine']")
    res_toolcall_ratio = max(res_toolcall_ratio, 0)
    toolcall_reward *= float(res_toolcall_ratio)
    outcome_reward = verification_reward
    final_reward = outcome_reward + toolcall_reward - format_penalties - evidence_freq_penalties

    print(f"ğŸ†ğŸ†ğŸ†<start>\nå·²è¯„ä¼°æ— å¼‚å¸¸: {no_exception}, outcome_reward: {verification_reward:.2f}, res_toolcall_ratio: {res_toolcall_ratio:.2f}, toolcall_reward: {toolcall_reward:.2f}, format_penalties: {format_penalties:.2f}, evidence_freq_penalties: {evidence_freq_penalties:.2f}, final_reward: {final_reward:.2f}")
    print(f"è§£æä¿¡æ¯: last_tool_is_submit={parsed_info['last_tool_is_submit']}, "
          f"submit_evidences_valid={parsed_info['submit_evidences_valid']}, "
          f"round_analysis_valid={parsed_info['round_analysis_valid']}, "
          f"round_sequence_valid={parsed_info['round_sequence_valid']}, "
          f"num_toolcalls={num_toolcalls}")
    print(f"[blue]<X> æ ¼å¼æƒ©ç½š: {format_penalties:.2f}, éªŒè¯å¥–åŠ±: {outcome_reward:.2f}, æœ€ç»ˆå¥–åŠ±: {final_reward:.2f}[/blue]")
    print(f"{N}æ¬¡éªŒè¯ä¸­å®Œå…¨æˆåŠŸ{success_count}æ¬¡ï¼Œæœ‰æ•ˆè¯æ®{valid_evidence_count}æ¬¡\n<end>ğŸ†ğŸ†ğŸ†")
    
    # return final_reward
    res = {
        "score": final_reward,
        "acc": acc,
    }
    return res
    