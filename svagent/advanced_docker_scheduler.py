# pylint: disable=line-too-long, function-name-too-long
import os
import sys
import asyncio
import time
import json
import logging
import requests
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional
from enum import Enum
from concurrent.futures import TimeoutError
import httpx
from aiohttp import web



# æ·»åŠ é¡¹ç›®è·¯å¾„ä»¥æ”¯æŒç›´æ¥è¿è¡Œ
if __name__ == "__main__":
    # This path adjustment is for direct execution of the script.
    # It might need to be adapted depending on the project structure.
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    # Assuming docker_client_tione is in the same package `svagent`
    from .docker_client_tione import TioneEnvManager
except (ImportError, ModuleNotFoundError):
    # Fallback for direct script execution
    from svagent.docker_client_tione import TioneEnvManager

# --- Logging Setup ---
# Use a logger that can be shared across modules
logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('docker_scheduler.log'),
            logging.StreamHandler()
        ]
    )

# --- Core Scheduler Classes (migrated from advanced_docker_scheduler.py) ---

class InstanceStatus(Enum):
    """å®ä¾‹çŠ¶æ€æšä¸¾"""
    CREATING = "creating"
    TESTING = "testing"
    READY = "ready"
    ALLOCATED = "allocated"
    DESTROYING = "destroying"
    FAILED = "failed"

@dataclass
class DockerInstance:
    """Dockerå®ä¾‹æ•°æ®ç±»"""
    instance_id: str
    env_id: str
    endpoint: str
    status: InstanceStatus
    created_at: datetime
    allocated_at: Optional[datetime] = None
    client_id: Optional[str] = None
    last_health_check: Optional[datetime] = None
    error_message: str = ""
    
    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        data = asdict(self)
        for key, value in data.items():
            if isinstance(value, datetime):
                data[key] = value.isoformat()
            elif isinstance(value, InstanceStatus):
                data[key] = value.value
        return data

class DockerScheduler:
    """é«˜çº§Dockerè°ƒåº¦å™¨"""
    
    def __init__(self, 
                 pool_size: int = 3,
                 max_pool_size: int = 10,
                 api_test_timeout: int = 900,
                 health_check_interval: int = 1200,
                 resource_config: dict = None):
        self.pool_size = pool_size
        self.max_pool_size = max_pool_size
        self.api_test_timeout = api_test_timeout
        self.health_check_interval = health_check_interval
        self.resource_config = resource_config or {'Cpu': 4000, 'Memory': 8000}
        
        self.instances: Dict[str, DockerInstance] = {}
        self.ready_queue: Optional[asyncio.Queue] = None
        self.loop: Optional[asyncio.AbstractEventLoop] = None
        
        self.running = False
        self.stats = {
            'total_created': 0,
            'total_allocated': 0,
            'total_destroyed': 0,
            'total_failed': 0,
            'start_time': datetime.now()
        }
        
    async def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨åå°ä»»åŠ¡"""
        if self.running:
            logger.info("Scheduler is already running.")
            return
            
        logger.info(f"ğŸš€ å¯åŠ¨Dockerè°ƒåº¦å™¨ - ç›®æ ‡æ± å¤§å°: {self.pool_size}")
        self.running = True
        self.loop = asyncio.get_running_loop()
        
        # Initialize the queue in the same event loop
        self.ready_queue = asyncio.Queue()

        # Start background tasks
        self.background_tasks = [
            asyncio.create_task(self._pool_manager()),
            asyncio.create_task(self._health_checker()),
            asyncio.create_task(self._stats_reporter())
        ]

    async def shutdown(self):
        """å…³é—­è°ƒåº¦å™¨å¹¶æ¸…ç†èµ„æº"""
        logger.info("ğŸ›‘ å¼€å§‹å…³é—­è°ƒåº¦å™¨...")
        self.running = False

        # Cancel background tasks
        for task in self.background_tasks:
            task.cancel()
        await asyncio.gather(*self.background_tasks, return_exceptions=True)
        
        # Destroy all existing instances
        destroy_tasks = [
            self._destroy_instance(instance_id) 
            for instance_id in list(self.instances.keys())
        ]
        if destroy_tasks:
            await asyncio.gather(*destroy_tasks, return_exceptions=True)
        
        logger.info("âœ… è°ƒåº¦å™¨å·²å…³é—­")

    async def _create_instance(self) -> Optional[DockerInstance]:
        instance_id = f"inst_{int(time.time())}_{os.urandom(4).hex()}"
        instance = DockerInstance(
            instance_id=instance_id, env_id="", endpoint="",
            status=InstanceStatus.CREATING, created_at=datetime.now()
        )
        self.instances[instance_id] = instance
        
        try:
            logger.info(f"ğŸ”¨ å¼€å§‹åˆ›å»ºå®ä¾‹: {instance_id}")
            manager = TioneEnvManager(type='OS')
            manager.create_params['ResourceInfo'] = self.resource_config.copy()
            
            start_time = time.time()
            env_result = await manager.create_env()
            creation_time = time.time() - start_time
            
            env_id = env_result.get('env_id')
            endpoint = env_result.get('endpoint')
            
            if not env_id or not endpoint:
                raise Exception(f"åˆ›å»ºå¤±è´¥ï¼Œè¿”å›ä¿¡æ¯ä¸å®Œæ•´: {env_result}")

            logger.info(f"ğŸ—ï¸ å®ä¾‹ {instance_id} ç¯å¢ƒåˆ›å»ºå®Œæˆï¼Œè€—æ—¶: {creation_time:.1f}s")
            
            instance.env_id = env_id
            instance.endpoint = endpoint
            instance.status = InstanceStatus.TESTING
            self.stats['total_created'] += 1
            
            if await self._test_instance_api(instance):
                instance.status = InstanceStatus.READY
                instance.last_health_check = datetime.now()
                await self.ready_queue.put(instance_id)
                total_time = time.time() - start_time
                logger.info(f"âœ… å®ä¾‹ {instance_id} å°±ç»ªï¼Œæ€»è€—æ—¶: {total_time:.1f}s")
                return instance
            else:
                raise Exception("APIæµ‹è¯•å¤±è´¥")
                
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºå®ä¾‹ {instance_id} å¤±è´¥: {e}")
            instance.status = InstanceStatus.FAILED
            instance.error_message = str(e)
            self.stats['total_failed'] += 1
            await self._destroy_instance(instance_id)
            return None
    
    async def _test_instance_api(self, instance: DockerInstance) -> bool:
        logger.info(f"ğŸ§ª æµ‹è¯•å®ä¾‹ {instance.instance_id} API at {instance.endpoint}")
        url = f"http://{instance.endpoint}/start"
        payload = {"avd_name": "Pixel_7_Pro_API_33"}

        loop = asyncio.get_running_loop()
        async def make_request():
            # requests is blocking, run it in an executor
            return await loop.run_in_executor(
                None, 
                lambda: requests.post(url, json=payload, timeout=120)
            )

        start_time = time.time()
        while time.time() - start_time < self.api_test_timeout:
            try:
                response = await make_request()
                if response.status_code in [200, 201, 202]:
                    logger.info(f"âœ… å®ä¾‹ {instance.instance_id} APIæµ‹è¯•æˆåŠŸ")
                    return True
            except requests.exceptions.RequestException:
                await asyncio.sleep(5)
            except Exception as e:
                logger.error(f"API test unexpected error for {instance.instance_id}: {e}")
                break # Exit on unexpected error
        
        logger.error(f"âŒ å®ä¾‹ {instance.instance_id} APIæµ‹è¯•è¶…æ—¶æˆ–å¤±è´¥")
        return False
    
    async def _destroy_instance(self, instance_id: str):
        instance = self.instances.get(instance_id)
        if not instance:
            return
        
        instance.status = InstanceStatus.DESTROYING
        logger.info(f"ğŸ—‘ï¸ é”€æ¯å®ä¾‹ {instance_id} (EnvId: {instance.env_id})")
        
        try:
            if instance.env_id:
                manager = TioneEnvManager(type='OS')
                await manager.delete_env(instance.env_id)
                logger.info(f"âœ… å®ä¾‹ {instance_id} äº‘ç«¯ç¯å¢ƒé”€æ¯æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ é”€æ¯å®ä¾‹ {instance_id} äº‘ç«¯ç¯å¢ƒå¤±è´¥: {e}")
        finally:
            if instance_id in self.instances:
                del self.instances[instance_id]
                self.stats['total_destroyed'] += 1
                logger.info(f"ğŸ—‘ï¸ å®ä¾‹ {instance_id} å·²ä»æœ¬åœ°ç§»é™¤")
    
    async def _pool_manager(self):
        """æ± ç®¡ç†å™¨ - åˆå§‹åŒ–å¹¶ç»´æŠ¤ç›®æ ‡æ± å¤§å°"""
        logger.info(f"ğŸ“¦ æ± ç®¡ç†å™¨å¯åŠ¨ - ç›®æ ‡æ± å¤§å°: {self.pool_size}")
        
        is_initial_startup = True
        
        while self.running:
            try:
                # ç»Ÿè®¡å½“å‰çŠ¶æ€
                ready_count = self.ready_queue.qsize()
                creating_count = sum(1 for inst in self.instances.values() if inst.status == InstanceStatus.CREATING)
                testing_count = sum(1 for inst in self.instances.values() if inst.status == InstanceStatus.TESTING)
                allocated_count = sum(1 for inst in self.instances.values() if inst.status == InstanceStatus.ALLOCATED)
                
                total_active = ready_count + creating_count + testing_count
                total_instances = len(self.instances)
                
                # æ ¹æ®å¯åŠ¨é˜¶æ®µé€‰æ‹©æ—¥å¿—çº§åˆ«
                if is_initial_startup and total_instances == 0:
                    logger.info(f"ğŸš€ å¼€å§‹åˆå§‹åŒ–å®ä¾‹æ± ï¼Œå¹¶è¡Œåˆ›å»º {self.pool_size} ä¸ªå®ä¾‹...")
                elif is_initial_startup:
                    logger.info(f"ğŸ” åˆå§‹åŒ–è¿›è¡Œä¸­ - å°±ç»ª:{ready_count} åˆ›å»ºä¸­:{creating_count} æµ‹è¯•ä¸­:{testing_count} æ€»è®¡:{total_instances}")
                else:
                    logger.debug(f"ğŸ” æ± çŠ¶æ€æ£€æŸ¥ - å°±ç»ª:{ready_count} åˆ›å»ºä¸­:{creating_count} æµ‹è¯•ä¸­:{testing_count} å·²åˆ†é…:{allocated_count} æ€»å®ä¾‹:{total_instances}")

                # å¦‚æœæ± ä¸­å®ä¾‹ä¸è¶³ï¼Œåˆ›å»ºæ–°å®ä¾‹
                needed = self.pool_size - total_active
                
                # é¢å¤–çš„å®‰å…¨æ£€æŸ¥ï¼šä¸èƒ½è¶…è¿‡æœ€å¤§æ± å¤§å°
                can_create = self.max_pool_size - total_instances
                
                # æ–°å¢ï¼šé™åˆ¶å¹¶å‘åˆ›å»ºæ•°é‡
                concurrent_creation_limit = 128
                ongoing_creations = creating_count + testing_count
                creation_slots_available = max(0, concurrent_creation_limit - ongoing_creations)
                
                actual_needed = min(needed, can_create, creation_slots_available)
                print(f"æ­£åœ¨åˆ›å»º/æµ‹è¯•çš„å®ä¾‹: {ongoing_creations}, åŒæ—¶åˆ›å»ºä¿¡å·é‡: {concurrent_creation_limit}, éœ€è¦åˆ›å»ºçš„æ•°é‡: {needed}, æœ¬æ¬¡å¯åŒæ—¶åˆ›å»ºçš„æ•°é‡: {can_create}")
                if actual_needed > 0:
                    if is_initial_startup:
                        logger.info(f"ğŸ“ˆ åˆå§‹åŒ–åˆ›å»º {actual_needed} ä¸ªæ–°å®ä¾‹")
                    else:
                        logger.info(f"ğŸ“ˆ æ± ä¸­å®ä¾‹ä¸è¶³ - ç›®æ ‡:{self.pool_size} å½“å‰æ´»è·ƒ:{total_active} éœ€è¦åˆ›å»º:{actual_needed} ä¸ªæ–°å®ä¾‹")
                    
                    tasks = [self._create_instance() for _ in range(actual_needed)]
                    await asyncio.gather(*tasks)

                elif total_instances >= self.max_pool_size:
                    if not is_initial_startup:
                        logger.warning(f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§æ± å¤§å°é™åˆ¶ {self.max_pool_size}ï¼Œä¸å†åˆ›å»ºæ–°å®ä¾‹")
                elif ongoing_creations >= concurrent_creation_limit:
                    logger.info(f"â³ å¹¶å‘åˆ›å»ºè¾¾åˆ°ä¸Šé™({concurrent_creation_limit})ï¼Œç­‰å¾…ç°æœ‰å®ä¾‹å®Œæˆ...")

                # æ£€æŸ¥åˆå§‹åŒ–æ˜¯å¦å®Œæˆ
                if is_initial_startup and ready_count >= self.pool_size:
                    is_initial_startup = False
                    logger.info(f"âœ… åˆå§‹åŒ–é˜¶æ®µå®Œæˆ - å°±ç»ªå®ä¾‹:{ready_count} æ€»å®ä¾‹:{total_instances}")
                
                # è°ƒæ•´æ£€æŸ¥é—´éš”
                sleep_time = 3 if is_initial_startup else 10
                await asyncio.sleep(sleep_time)

            except asyncio.CancelledError:
                logger.info("æ± ç®¡ç†å™¨è¢«å–æ¶ˆ")
                break
            except Exception as e:
                logger.error(f"æ± ç®¡ç†å™¨é”™è¯¯: {e}", exc_info=True)
                await asyncio.sleep(10)
    

    async def _health_checker(self):
        """å¥åº·æ£€æŸ¥å™¨"""
        while self.running:
            try:
                await asyncio.sleep(self.health_check_interval)
                current_time = datetime.now()
                for instance in list(self.instances.values()):
                    if instance.status == InstanceStatus.READY:
                        if (instance.last_health_check is None or 
                            current_time - instance.last_health_check > timedelta(seconds=self.health_check_interval)):
                            asyncio.create_task(self._check_instance_health(instance))
            except asyncio.CancelledError:
                logger.info("å¥åº·æ£€æŸ¥å™¨è¢«å–æ¶ˆ")
                break
            except Exception as e:
                logger.error(f"å¥åº·æ£€æŸ¥å™¨é”™è¯¯: {e}", exc_info=True)
                await asyncio.sleep(10)


    async def _check_instance_health(self, instance: DockerInstance):
        """æ£€æŸ¥å•ä¸ªå®ä¾‹å¥åº·çŠ¶æ€"""
        logger.debug(f"ğŸ©º å¥åº·æ£€æŸ¥å®ä¾‹: {instance.instance_id}")
        try:
            url = f"http://{instance.endpoint}/start"
            payload = {"avd_name": "Pixel_7_Pro_API_33"}

            # ä½¿ç”¨httpxæ¥åˆ›å»º
            # async with httpx.AsyncClient(timeout=120.0) as client:  # æ˜ç¡®æŒ‡å®šç§’
            #     response = await client.post(
            #         f"http://{instance.endpoint}/start",
            #         json={"avd_name": "Pixel_7_Pro_API_33"}
            #     )
            #     response.raise_for_status()  # è‡ªåŠ¨å¤„ç†4xx/5xx

            loop = asyncio.get_running_loop()
            response = await loop.run_in_executor(
                None, 
                lambda: requests.post(url, json=payload, timeout=120)
            )
            if response.status_code in [200, 201, 202]:
                instance.last_health_check = datetime.now()
                logger.debug(f"âœ… å®ä¾‹ {instance.instance_id} å¥åº·æ£€æŸ¥é€šè¿‡")
            else:
                logger.warning(f"âš ï¸ å®ä¾‹ {instance.instance_id} å¥åº·æ£€æŸ¥å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                await self._handle_unhealthy_instance(instance)
                
        except Exception as e:
            logger.warning(f"âš ï¸ å®ä¾‹ {instance.instance_id} å¥åº·æ£€æŸ¥å¼‚å¸¸: {e}")
            await self._handle_unhealthy_instance(instance)

    async def _handle_unhealthy_instance(self, instance: DockerInstance):
        """å¤„ç†ä¸å¥åº·çš„å®ä¾‹"""
        logger.warning(f"ğŸ”„ å¤„ç†ä¸å¥åº·å®ä¾‹: {instance.instance_id}")
        
        # ä»å°±ç»ªé˜Ÿåˆ—ä¸­ç§»é™¤ (å¦‚æœå­˜åœ¨)
        # è¿™æ˜¯ä¸€ä¸ªå°½åŠ›è€Œä¸ºçš„æ“ä½œï¼Œå› ä¸ºasyncio.Queueæ²¡æœ‰ç›´æ¥çš„removeæ–¹æ³•
        temp_queue = asyncio.Queue()
        removed = False
        while not self.ready_queue.empty():
            item = await self.ready_queue.get()
            if item == instance.instance_id and not removed:
                removed = True
                logger.info(f"ğŸ”ª ä»å°±ç»ªé˜Ÿåˆ—ä¸­ç§»é™¤ä¸å¥åº·å®ä¾‹: {instance.instance_id}")
            else:
                await temp_queue.put(item)
        
        # å°†å…¶ä»–å®ä¾‹æ”¾å›åŸé˜Ÿåˆ—
        while not temp_queue.empty():
            await self.ready_queue.put(await temp_queue.get())
            
        # é”€æ¯å®ä¾‹
        await self._destroy_instance(instance.instance_id)
    
    async def _stats_reporter(self):
        while self.running:
            try:
                await asyncio.sleep(60)
                self._log_stats()
            except asyncio.CancelledError:
                logger.info("ç»Ÿè®¡æŠ¥å‘Šå™¨è¢«å–æ¶ˆ")
                break
            except Exception as e:
                logger.error(f"ç»Ÿè®¡æŠ¥å‘Šå™¨é”™è¯¯: {e}", exc_info=True)
    
    def _log_stats(self):
        status_counts = {status.value: 0 for status in InstanceStatus}
        for instance in self.instances.values():
            status_counts[instance.status.value] += 1
        
        uptime = datetime.now() - self.stats['start_time']

        logger.info(f"ğŸ“Š è°ƒåº¦å™¨çŠ¶æ€ - å°±ç»ª:{self.ready_queue.qsize()} å·²åˆ†é…:{status_counts['allocated']} "
                    f"åˆ›å»ºä¸­:{status_counts['creating']} æµ‹è¯•ä¸­:{status_counts['testing']} "
                    f"å¤±è´¥:{status_counts['failed']} æ€»å®ä¾‹:{len(self.instances)}")
        logger.info(f"ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯ - æ€»åˆ›å»º:{self.stats['total_created']} æ€»åˆ†é…:{self.stats['total_allocated']} "
                    f"æ€»é”€æ¯:{self.stats['total_destroyed']} è¿è¡Œæ—¶é—´:{uptime}")
    
    # --- Client API Methods ---

    async def allocate_instance(self, client_id: str) -> Optional[Dict]:
        logger.info(f"ğŸ¯ å®¢æˆ·ç«¯ {client_id} è¯·æ±‚åˆ†é…å®ä¾‹")

        current_loop = asyncio.get_running_loop()
        timeout = 3

        try:
            # If the current coroutine is running on a different event loop
            # from the one the scheduler started in (e.g., aiohttp worker),
            # we must use thread-safe methods to interact with the queue.
            if self.loop and current_loop is not self.loop:
                future = asyncio.run_coroutine_threadsafe(self.ready_queue.get(), self.loop)
                # future.result() will raise TimeoutError from concurrent.futures
                instance_id = future.result(timeout=timeout)
            else:
                # Running in the same event loop, we can await directly.
                # asyncio.wait_for will raise asyncio.TimeoutError.
                instance_id = await asyncio.wait_for(self.ready_queue.get(), timeout=timeout)
            
            instance = self.instances.get(instance_id)
            if not instance or instance.status != InstanceStatus.READY:
                logger.warning(f"Dequeued instance {instance_id} not ready, requeueing.")
                # Use thread-safe put if necessary
                if self.loop and current_loop is not self.loop:
                    asyncio.run_coroutine_threadsafe(self.ready_queue.put(instance_id), self.loop)
                else:
                    await self.ready_queue.put(instance_id)
                return None

            instance.status = InstanceStatus.ALLOCATED
            instance.allocated_at = datetime.now()
            instance.client_id = client_id
            self.stats['total_allocated'] += 1
            
            result = instance.to_dict()
            logger.info(f"âœ… ä¸ºå®¢æˆ·ç«¯ {client_id} åˆ†é…å®ä¾‹ {instance_id}")
            return result
            
        except (asyncio.TimeoutError, TimeoutError): # Handle both timeout types
            logger.warning(f"â° å®¢æˆ·ç«¯ {client_id} åˆ†é…å®ä¾‹è¶…æ—¶")
            return None
        
        except Exception as e:
            logger.error(f"âŒ åˆ†é…å®ä¾‹ç»™å®¢æˆ·ç«¯ {client_id} æ—¶å‡ºé”™: {e}", exc_info=True)
            return None
    
    async def release_instance(self, instance_id: str, client_id: str) -> bool:
        logger.info(f"ğŸ”„ å®¢æˆ·ç«¯ {client_id} é‡Šæ”¾å®ä¾‹ {instance_id}")
        instance = self.instances.get(instance_id)
        
        if not instance:
            logger.warning(f"âš ï¸ å®ä¾‹ {instance_id} ä¸å­˜åœ¨")
            return False
        
        if instance.client_id != client_id:
            logger.warning(f"âš ï¸ å®¢æˆ·ç«¯ {client_id} æ— æƒé‡Šæ”¾å®ä¾‹ {instance_id}")
            return False
        
        # Destroy the instance and let the pool manager create a new one
        await self._destroy_instance(instance_id)
        return True
    
    async def get_status_async(self) -> Dict:
        """è·å–è°ƒåº¦å™¨çŠ¶æ€ (å¼‚æ­¥)"""
        status_counts = {status.value: 0 for status in InstanceStatus}
        for instance in self.instances.values():
            status_counts[instance.status.value] += 1

        return {
            'running': self.running,
            'pool_size': self.pool_size,
            'max_pool_size': self.max_pool_size,
            'ready_count': self.ready_queue.qsize(),
            'allocated_count': status_counts['allocated'],
            'creating_count': status_counts['creating'],
            'testing_count': status_counts['testing'],
            'total_instances': len(self.instances),
            'stats': self.stats,
            'uptime': (datetime.now() - self.stats['start_time']).total_seconds()
        }
    
    async def get_instances_async(self) -> List[Dict]:
        """è·å–æ‰€æœ‰å®ä¾‹ä¿¡æ¯ (å¼‚æ­¥)"""
        return [inst.to_dict() for inst in self.instances.values()]

# --- aiohttp App Handlers ---

async def get_status(request: web.Request):
    """è·å–è°ƒåº¦å™¨çŠ¶æ€"""
    scheduler = request.app['scheduler']
    status = await scheduler.get_status_async()
    return web.json_response({'success': True, 'data': status})

async def get_instances(request: web.Request):
    """è·å–æ‰€æœ‰å®ä¾‹ä¿¡æ¯"""
    scheduler = request.app['scheduler']
    instances = await scheduler.get_instances_async()
    return web.json_response({'success': True, 'data': instances})

async def allocate_instance_handler(request: web.Request):
    """ä¸ºå®¢æˆ·ç«¯åˆ†é…å®ä¾‹"""
    scheduler = request.app['scheduler']
    data = await request.json()
    client_id = data.get('client_id', 'unknown')
    
    result = await scheduler.allocate_instance(client_id)
    
    if result:
        return web.json_response({'success': True, 'data': result})
    else:
        return web.json_response(
            {'success': False, 'error': 'No available instances, please try again later.'},
            status=503
        )

async def release_instance_handler(request: web.Request):
    """å®¢æˆ·ç«¯é‡Šæ”¾å®ä¾‹"""
    scheduler = request.app['scheduler']
    data = await request.json()
    instance_id = data.get('instance_id')
    client_id = data.get('client_id')

    if not instance_id or not client_id:
        return web.json_response(
            {'success': False, 'error': 'instance_id and client_id are required'},
            status=400
        )

    success = await scheduler.release_instance(instance_id, client_id)
    
    if success:
        return web.json_response({'success': True})
    else:
        return web.json_response(
            {'success': False, 'error': 'Failed to release instance.'},
            status=400
        )

async def on_startup(app: web.Application):
    """aiohttp startup handler"""
    scheduler = app['scheduler']
    # The scheduler's background tasks are started here, within the
    # event loop that aiohttp will use for handling requests.
    asyncio.create_task(scheduler.start())
    logger.info("ğŸš€ Scheduler background tasks started.")

async def on_shutdown(app: web.Application):
    """aiohttp shutdown handler"""
    scheduler = app['scheduler']
    await scheduler.shutdown()
    logger.info("âœ… Scheduler has been shut down.")

def create_app(scheduler: DockerScheduler) -> web.Application:
    """åˆ›å»ºå¹¶é…ç½®aiohttpåº”ç”¨"""
    app = web.Application()
    app['scheduler'] = scheduler
    
    app.router.add_get('/status', get_status)
    app.router.add_get('/instances', get_instances)
    app.router.add_post('/allocate', allocate_instance_handler)
    app.router.add_post('/release', release_instance_handler)
    
    app.on_startup.append(on_startup)
    app.on_shutdown.append(on_shutdown)
    
    return app

# ==================== ä¸»ç¨‹åº ====================

def main():
    """ä¸»ç¨‹åº"""
    import argparse
    
    parser = argparse.ArgumentParser(description='é«˜çº§Dockerè°ƒåº¦å™¨ (aiohttpç‰ˆ)')
    parser.add_argument('--pool-size', type=int, default=64, help='ç›®æ ‡æ± å¤§å°')
    parser.add_argument('--max-pool-size', type=int, default=96, help='æœ€å¤§æ± å¤§å°')
    parser.add_argument('--api-timeout', type=int, default=900, help='APIæµ‹è¯•è¶…æ—¶æ—¶é—´(ç§’)')
    parser.add_argument('--health-interval', type=int, default=1200, help='å¥åº·æ£€æŸ¥é—´éš”(ç§’)')
    parser.add_argument('--host', type=str, default='0.0.0.0', help='HTTP API Host')
    parser.add_argument('--port', type=int, default=8080, help='HTTP APIç«¯å£')
    parser.add_argument('--cpu', type=int, default=6000, help='CPUé…ç½®')
    parser.add_argument('--memory', type=int, default=12000, help='å†…å­˜é…ç½®')
    args = parser.parse_args()

    # 1. åˆ›å»ºè°ƒåº¦å™¨å®ä¾‹
    scheduler = DockerScheduler(
        pool_size=args.pool_size,
        max_pool_size=args.max_pool_size,
        api_test_timeout=args.api_timeout,
        health_check_interval=args.health_interval,
        resource_config={'Cpu': args.cpu, 'Memory': args.memory}
    )

    # 2. åˆ›å»ºaiohttpåº”ç”¨
    app = create_app(scheduler)

    # 3. è¿è¡Œåº”ç”¨
    # web.run_app handles graceful shutdown on Ctrl+C
    logger.info(f"ğŸŒ aiohttpæœåŠ¡å™¨å°†åœ¨ http://{args.host}:{args.port} ä¸Šè¿è¡Œ")
    web.run_app(app, host=args.host, port=args.port)


if __name__ == "__main__":
    main()
